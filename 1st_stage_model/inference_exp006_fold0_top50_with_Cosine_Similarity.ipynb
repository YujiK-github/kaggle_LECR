{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YujiK-github/kaggle_LECR/blob/main/1st_stage_model/1st_stage_model_inference_exp006_fold0_top50_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42qYTvHb_3ZO",
        "outputId": "e64ed9a4-3d12-46ec-d9e3-6bfa94ec0dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar  4 17:42:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dssL5X5NGwEQ",
        "outputId": "5a2ac5b8-e69a-4abe-f851-b37866edfd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m597.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=b64eb5107903a2c08967dbc6ba2c0cd053f90d9329176c061d53bc14102d9e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.10.1 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U sentence_transformers\n",
        "%pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZpIXalFNcW3",
        "outputId": "dd26cf5d-20dc-4251-ae6d-c4d3814ae4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOKLCtRkNdJT",
        "outputId": "d81c2661-8dac-4ae8-d62b-e9c035a31b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 1st_stage_model_inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile 1st_stage_model_inference.py\n",
        "\n",
        "# ===============================================================\n",
        "#  Library\n",
        "# ===============================================================\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import heapq\n",
        "import pickle\n",
        "import random\n",
        "import requests\n",
        "import argparse\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Union\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "import torch \n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig, DataCollatorWithPadding\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "# ===============================================================\n",
        "#  args\n",
        "# ===============================================================\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, required=False)\n",
        "    parser.add_argument(\"--input_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/\", required=False)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/last_data/\", required=False)\n",
        "    parser.add_argument(\"--filename\", type=str, default=\"exp006\", required=False)\n",
        "    parser.add_argument(\"--model\", type=str, default=\"sentence-transformers/all-mpnet-base-v2\", required=False)\n",
        "    parser.add_argument(\"--trn_fold\", type=int, default=0, required=False)\n",
        "    parser.add_argument(\"--n_splits\", type=int, default=3, required=False)\n",
        "    parser.add_argument(\"--max_len\", type=int, default=256, required=False)\n",
        "    parser.add_argument(\"--n_neighbors\", type=int, default=50, required=False)\n",
        "    parser.add_argument(\"--corpus_chunk_size\", type=int, default=10_000, required=False)\n",
        "    parser.add_argument(\"--mode\", type=str, default=\"for_validation\", choices=[\"for_validation\", \"for_training\"], required=False)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=96, required=False)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  Utils\n",
        "# ===============================================================\n",
        "\n",
        "# 任意のメッセージを通知する関数\n",
        "def send_slack_message_notification(message):\n",
        "    webhook_url = ' [URL] '  \n",
        "    data = json.dumps({'text': message})\n",
        "    headers = {'content-type': 'application/json'}\n",
        "    requests.post(webhook_url, data=data, headers=headers)\n",
        "\n",
        "\n",
        "def seed_everything(cfg):\n",
        "    \"\"\"set seed\"\"\"\n",
        "    random.seed(cfg.seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
        "    np.random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    torch.cuda.manual_seed(cfg.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    cited: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "\n",
        "# ===============================================================\n",
        "#  Data Loading\n",
        "# ===============================================================\n",
        "def data_load(cfg):\n",
        "    print(\"========== Data Loading ==========\")\n",
        "    df_content = pd.read_csv(cfg.input_dir+\"content.csv\").fillna({\"title\": \"\", \"description\": \"\", \"text\":\"\"})\n",
        "    df_topics = pd.read_csv(cfg.input_dir+\"topics.csv\").fillna({\"title\": \"\", \"description\": \"\"})\n",
        "    df_corr = pd.read_csv(cfg.input_dir+\"correlations.csv\")   \n",
        "    \n",
        "    df_content.rename(columns={\"id\":\"content_id\"}, inplace=True)\n",
        "    df_topics.rename(columns={\"id\":\"topic_id\"}, inplace=True)\n",
        "    df_corr.rename(columns={\"content_ids\":\"content_id\"}, inplace=True)\n",
        "    \n",
        "    cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model, is_fast=True)\n",
        "    \n",
        "    df_content[\"sentence\"] = df_content[\"title\"] + cfg.tokenizer.sep_token + df_content[\"description\"]\n",
        "    df_topics[\"sentence\"] = df_topics[\"title\"] + cfg.tokenizer.sep_token +  df_topics[\"description\"] +\\\n",
        "    cfg.tokenizer.sep_token + df_topics[\"context\"]\n",
        "    df_topics[\"sentence\"] = df_topics[\"sentence\"].str.replace(\" >> \",  \" \")\n",
        "    df_topics = pd.merge(df_topics, df_corr, on=\"topic_id\", how=\"left\")\n",
        "    \n",
        "    df_content[\"content_sentence\"] = df_content[\"sentence\"]\n",
        "    df_topics[\"topic_sentence\"] = df_topics[\"sentence\"]\n",
        "    \n",
        "    df_topics[\"content_id\"] = df_topics[\"content_id\"].str.split()\n",
        "    df_topics = df_topics.explode(\"content_id\", ignore_index=True)\n",
        "    \n",
        "    #df_topics = pd.merge(df_topics, df_content[[\"content_id\", \"content_sentence\"]], on=\"content_id\", how=\"left\")\n",
        "    \n",
        "    print(df_topics.shape)\n",
        "    # category == sourceのtopic\n",
        "    df_train = df_topics[(df_topics[\"category\"] == \"source\")&(df_topics[\"has_content\"] == True)].reset_index(drop=True)\n",
        "    \n",
        "    # category == \n",
        "    df_valid = df_topics[(df_topics[\"category\"] != \"source\")&(df_topics[\"has_content\"] == True)].reset_index(drop=True)    \n",
        "    \n",
        "    \"\"\"\n",
        "    split cv using GroupKFold\n",
        "    - split based on number of target's content_id or topic_id\n",
        "    \"\"\"\n",
        "    print(\"========== CV split ==========\")\n",
        "    gkf = GroupKFold(cfg.n_splits)\n",
        "    for i, (tr, val) in enumerate(gkf.split(X=df_valid, groups=df_valid[\"channel\"])):\n",
        "        df_valid.loc[val, \"fold\"] = i\n",
        "    print(df_valid.groupby(\"fold\").size())\n",
        "    print(df_valid[[\"topic_id\", \"fold\"]].drop_duplicates().groupby(\"fold\").size())\n",
        "    \n",
        "    _df_train = df_valid[df_valid[\"fold\"] != cfg.trn_fold].reset_index(drop=True)\n",
        "    df_valid = df_valid[df_valid[\"fold\"] == cfg.trn_fold].reset_index(drop=True)\n",
        "    \n",
        "    df_train = pd.concat([df_train, _df_train], ignore_index=True)\n",
        "\n",
        "    print(\"df_content\", df_content.shape)\n",
        "    print(\"df_corr\", df_corr.shape)\n",
        "    \n",
        "    print(\"Input Sentence Example\")\n",
        "    print(\"========== Topics ==========\")\n",
        "    print(df_topics[\"sentence\"].values.tolist()[:2])\n",
        "    print(\"========== Content ==========\")\n",
        "    print(df_content[\"sentence\"].values.tolist()[:2])\n",
        "\n",
        "    if cfg.mode == \"for_validation\":\n",
        "        df = df_valid\n",
        "    elif cfg.mode == \"for_training\":\n",
        "        df = df_train\n",
        "\n",
        "    print(\"df.shape: \", df.shape)\n",
        "\n",
        "    return df_content, df\n",
        "\n",
        "def prepare_valid(df_content, df_valid):\n",
        "    \"\"\"\n",
        "    Create a query and corpus like the folloing.\n",
        "    \n",
        "    ex)\n",
        "    queries = {'q1': 'What is machine learning?',\n",
        "               'q2': 'How does deep learning work?'}\n",
        "    corpus = {'d1': 'Machine learning is a method of data analysis.', \n",
        "              'd2': 'Deep learning is a subfield of machine learning.', \n",
        "              'd3': 'Neural networks are used in deep learning.'}\n",
        "    relevant_docs = {'q1': {'d1'}, \n",
        "                     'q2': {'d2', 'd3'}}\n",
        "\n",
        "    evaluator = evaluation.InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n",
        "    \"\"\"\n",
        "    queries = df_valid[[\"topic_id\", \"topic_sentence\"]].set_index('topic_id').to_dict()['topic_sentence']\n",
        "    corpus = df_content[[\"content_id\", \"content_sentence\"]].set_index('content_id').to_dict()['content_sentence']\n",
        "    relevant_docs =  df_valid.groupby('topic_id')['content_id'].apply(set).to_dict()\n",
        "    return queries, corpus, relevant_docs\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  Convert sentence to embeddings\n",
        "# ===============================================================\n",
        "def FeatureExtractor(cfg):\n",
        "    word_embedding_model = models.Transformer(cfg.model, max_seq_length=cfg.max_len)\n",
        "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_pair(cfg, queries: dict, corpus: dict, model, device):\n",
        "    \"\"\"\n",
        "    https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/evaluation/InformationRetrievalEvaluator.py\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    queries_ids = list(queries.keys())\n",
        "    queries = [queries[qid] for qid in queries_ids]\n",
        "\n",
        "    corpus_ids = list(corpus.keys())\n",
        "    corpus = [corpus[cid] for cid in corpus_ids]\n",
        "\n",
        "    query_embeddings = model.encode(queries,  \n",
        "                                    batch_size=cfg.batch_size, \n",
        "                                    convert_to_tensor=True)\n",
        "\n",
        "    queries_result_list = [[] for _ in range(len(query_embeddings))]\n",
        "\n",
        "    for corpus_start_idx in tqdm(range(0, len(corpus), cfg.corpus_chunk_size)):\n",
        "        corpus_end_idx = min(corpus_start_idx + cfg.corpus_chunk_size, len(corpus))\n",
        "\n",
        "        sub_corpus_embeddings = model.encode(corpus[corpus_start_idx:corpus_end_idx], \n",
        "                                             show_progress_bar=False, \n",
        "                                             batch_size=cfg.batch_size, \n",
        "                                             convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarites\n",
        "        pair_scores = cos_sim(query_embeddings, sub_corpus_embeddings)\n",
        "\n",
        "        # Get top-k values\n",
        "        pair_scores_top_k_values, pair_scores_top_k_idx = torch.topk(pair_scores, \n",
        "                                                                     min(cfg.n_neighbors, len(pair_scores[0])), \n",
        "                                                                     dim=1, largest=True, sorted=False)\n",
        "        \n",
        "        pair_scores_top_k_values = pair_scores_top_k_values.cpu().tolist()\n",
        "        pair_scores_top_k_idx = pair_scores_top_k_idx.cpu().tolist()\n",
        "\n",
        "        for query_itr in range(len(query_embeddings)):\n",
        "            for sub_corpus_id, score in zip(pair_scores_top_k_idx[query_itr], pair_scores_top_k_values[query_itr]):\n",
        "                corpus_id = corpus_ids[corpus_start_idx+sub_corpus_id]\n",
        "                if len(queries_result_list[query_itr]) < cfg.n_neighbors:\n",
        "                    heapq.heappush(queries_result_list[query_itr], (score, corpus_id))  # heaqp tracks the quantity of the first element in the tuple\n",
        "                else:\n",
        "                    heapq.heappushpop(queries_result_list[query_itr], (score, corpus_id))\n",
        "\n",
        "    for query_itr in range(len(queries_result_list)):\n",
        "        for doc_itr in range(len(queries_result_list[query_itr])):\n",
        "            score, corpus_id = queries_result_list[query_itr][doc_itr]\n",
        "            queries_result_list[query_itr][doc_itr] = {'corpus_id': corpus_id, 'score': score}\n",
        "    return queries_ids, queries_result_list\n",
        "\n",
        "\n",
        "def save_pair(cfg, queries_ids, queries_result_list):\n",
        "    pair = {}\n",
        "    for query_itr in range(len(queries_result_list)):\n",
        "        query_id = queries_ids[query_itr]\n",
        "        # Sort scores\n",
        "        top_hits = sorted(queries_result_list[query_itr], key=lambda x: x['score'], reverse=True)\n",
        "        corpus_id_list = [(d['corpus_id'], d['score']) for d in top_hits[0:cfg.n_neighbors]]\n",
        "        pair[query_id] = corpus_id_list\n",
        "\n",
        "    if cfg.mode == \"for_validation\":\n",
        "        path = cfg.output_dir+f\"validation_top_{cfg.n_neighbors}_ver2.pkl\"\n",
        "    elif cfg.mode == \"for_training\":\n",
        "        path = cfg.output_dir+f\"train_top_{cfg.n_neighbors}_ver3.pkl\"\n",
        "\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(pair, f)\n",
        "    print(f\"{path} saved!\")\n",
        "\n",
        "\n",
        "def get_recall(cfg, queries_ids, queries_result_list, relevant_docs):\n",
        "    recall_at_k = []\n",
        "\n",
        "    for query_itr in range(len(queries_result_list)):\n",
        "        query_id = queries_ids[query_itr]\n",
        "        # Sort scores\n",
        "        top_hits = sorted(queries_result_list[query_itr], key=lambda x: x['score'], reverse=True)\n",
        "        query_relevant_docs = relevant_docs[query_id]\n",
        "        \n",
        "        num_correct = 0\n",
        "        for hit in top_hits[0:cfg.n_neighbors]:\n",
        "            if hit['corpus_id'] in query_relevant_docs:\n",
        "                num_correct += 1\n",
        "\n",
        "        recall_at_k.append(num_correct / len(query_relevant_docs))\n",
        "    recall_at_k = np.mean(recall_at_k)\n",
        "    return recall_at_k\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  main\n",
        "# ===============================================================\n",
        "def main(cfg):\n",
        "    seed_everything(cfg)\n",
        "    df_content, df_topics = data_load(cfg)\n",
        "    model = FeatureExtractor(cfg)\n",
        "    model.to(device)\n",
        "    queries, corpus, relevant_docs = prepare_valid(df_content, df_topics)\n",
        "    queries_ids, queries_result_list = get_pair(cfg, queries, corpus, model, device)\n",
        "    save_pair(cfg, queries_ids, queries_result_list)\n",
        "    score = get_recall(cfg, queries_ids, queries_result_list, relevant_docs)\n",
        "    print(score)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  Execute\n",
        "# ===============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "\n",
        "    \n",
        "    args.output_dir = args.output_dir + \"1st/\" + args.filename + f\"/fold{str(args.trn_fold)}/\"\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    args.model = \"/content/drive/MyDrive/KAGGLE-LECR/last_data/1st/\" + args.filename + f\"/fold{str(args.trn_fold)}/\" \\\n",
        "                    + args.model.replace(\"/\", \"-\") + \"_fine-tuned/\"\n",
        "\n",
        "    for k, v in vars(args).items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    main(args)\n",
        "    send_slack_message_notification(f\"[{args.filename} : {args.trn_fold} : {args.mode}] finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1st_stage_model_inference.py\\\n",
        "--trn_fold 0\\\n",
        "--n_neighbors 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uldev75gcX2k",
        "outputId": "a95ac4d5-f465-424a-d733-0ba09b383bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 17:43:33.761537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 17:43:33.916826: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-04 17:43:34.675028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 17:43:34.675137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 17:43:34.675157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "device: cuda\n",
            "seed: 42\n",
            "input_dir: /content/drive/MyDrive/KAGGLE-LECR/\n",
            "output_dir: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/\n",
            "filename: exp006\n",
            "model: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/sentence-transformers-all-mpnet-base-v2_fine-tuned/\n",
            "trn_fold: 0\n",
            "n_splits: 3\n",
            "max_len: 256\n",
            "n_neighbors: 50\n",
            "corpus_chunk_size: 10000\n",
            "mode: for_validation\n",
            "batch_size: 96\n",
            "========== Data Loading ==========\n",
            "(295374, 14)\n",
            "========== CV split ==========\n",
            "fold\n",
            "0.0    34239\n",
            "1.0    34194\n",
            "2.0    34196\n",
            "dtype: int64\n",
            "fold\n",
            "0.0     7190\n",
            "1.0     6995\n",
            "2.0    10818\n",
            "dtype: int64\n",
            "df_content (154047, 10)\n",
            "df_corr (61517, 2)\n",
            "Input Sentence Example\n",
            "========== Topics ==========\n",
            "['Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите', 'Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите']\n",
            "========== Content ==========\n",
            "['Sumar números de varios dígitos: 48,029+233,930 </s>Suma 48,029+233,930 mediante el algoritmo estándar.\\n\\n', 'Trovare i fattori di un numero</s>Sal trova i fattori di 120.\\n\\n']\n",
            "df.shape:  (34239, 15)\n",
            "100% 16/16 [02:42<00:00, 10.17s/it]\n",
            "/content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/validation_top_50_ver2.pkl saved!\n",
            "0.8502826807828325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxOyovmS6otj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01b8282-9861-404a-9417-b07979ef4b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 17:47:15.237672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 17:47:15.394007: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-04 17:47:16.160830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 17:47:16.160956: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 17:47:16.160975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "device: cuda\n",
            "seed: 42\n",
            "input_dir: /content/drive/MyDrive/KAGGLE-LECR/\n",
            "output_dir: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/\n",
            "filename: exp006\n",
            "model: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/sentence-transformers-all-mpnet-base-v2_fine-tuned/\n",
            "trn_fold: 0\n",
            "n_splits: 3\n",
            "max_len: 256\n",
            "n_neighbors: 50\n",
            "corpus_chunk_size: 10000\n",
            "mode: for_training\n",
            "batch_size: 96\n",
            "========== Data Loading ==========\n",
            "(295374, 14)\n",
            "========== CV split ==========\n",
            "fold\n",
            "0.0    34239\n",
            "1.0    34194\n",
            "2.0    34196\n",
            "dtype: int64\n",
            "fold\n",
            "0.0     7190\n",
            "1.0     6995\n",
            "2.0    10818\n",
            "dtype: int64\n",
            "df_content (154047, 10)\n",
            "df_corr (61517, 2)\n",
            "Input Sentence Example\n",
            "========== Topics ==========\n",
            "['Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите', 'Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите']\n",
            "========== Content ==========\n",
            "['Sumar números de varios dígitos: 48,029+233,930 </s>Suma 48,029+233,930 mediante el algoritmo estándar.\\n\\n', 'Trovare i fattori di un numero</s>Sal trova i fattori di 120.\\n\\n']\n",
            "df.shape:  (245680, 15)\n",
            "Traceback (most recent call last):\n",
            "  File \"1st_stage_model_inference.py\", line 326, in <module>\n",
            "    main(args)\n",
            "  File \"1st_stage_model_inference.py\", line 305, in main\n",
            "    queries_ids, queries_result_list = get_pair(cfg, queries, corpus, model, device)\n",
            "  File \"1st_stage_model_inference.py\", line 218, in get_pair\n",
            "    query_embeddings = model.encode(queries,  \n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sentence_transformers/SentenceTransformer.py\", line 162, in encode\n",
            "    features = batch_to_device(features, device)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sentence_transformers/util.py\", line 300, in batch_to_device\n",
            "    batch[key] = batch[key].to(target_device)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python 1st_stage_model_inference.py\\\n",
        "--mode for_training\\\n",
        "--trn_fold 0\\\n",
        "--n_neighbors 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWYeq6Ic_lb_"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98I2TxNPvKuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
