{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YujiK-github/kaggle_LECR/blob/main/1st_stage_model/1st_stage_model_train_exp006_fold0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "v1LZrzrkp9eS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9dc123-e68d-4890-b45a-28961d8ed059"
      },
      "id": "v1LZrzrkp9eS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  2 15:47:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebC_ggpE05cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c07120-562a-42c4-b0ae-4b0e837872f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ebC_ggpE05cj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f33322f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8ca05c-02e2-40db-df83-b0da858205b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=bb9b5c225fd75269b7ff810c21b52db39ab8f3a241cc5ed16a643db620a10899\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, multiprocess, responses, datasets\n",
            "Successfully installed datasets-2.10.1 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install datasets transformers[sentencepiece]"
      ],
      "id": "0f33322f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f6c2cef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bddcdd1-c68f-44c0-aea0-6dd5ca6e33ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 1st_stage_model_train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile 1st_stage_model_train.py\n",
        "\n",
        "# ===============================================================\n",
        "#  Library\n",
        "# ===============================================================\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "import glob\n",
        "import random\n",
        "import argparse\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import heapq\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig, DataCollatorWithPadding\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation, datasets\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import stat\n",
        "from collections import OrderedDict\n",
        "from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional\n",
        "import requests\n",
        "from sentence_transformers import __MODEL_HUB_ORGANIZATION__\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "from sentence_transformers.util import import_from_string, batch_to_device, fullname, snapshot_download\n",
        "from sentence_transformers.models import Transformer, Pooling, Dense\n",
        "from sentence_transformers.model_card_templates import ModelCardTemplate\n",
        "from sentence_transformers import __version__\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "from collections import OrderedDict\n",
        "from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional\n",
        "import requests\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import transformers\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download\n",
        "import torch\n",
        "from torch import nn, Tensor, device\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "from tqdm.autonotebook import trange\n",
        "import math\n",
        "import queue\n",
        "import tempfile\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "# ===============================================================\n",
        "#  args\n",
        "# ===============================================================\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, required=False)\n",
        "    parser.add_argument(\"--input_dir\", type=str, \n",
        "                        default=\"/content/drive/MyDrive/KAGGLE-LECR/\", \n",
        "                        required=False)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/\", required=False)\n",
        "    parser.add_argument(\"--model\", type=str, \n",
        "                        default=\"sentence-transformers/all-mpnet-base-v2\", required=False)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=768, required=False)\n",
        "    parser.add_argument(\"--max_len\", type=int, default=128, required=False)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, required=False)\n",
        "    parser.add_argument(\"--trn_fold\", type=int, default=0, required=False)\n",
        "    parser.add_argument(\"--n_splits\", type=int, default=3, required=False)\n",
        "    parser.add_argument(\"--print_freq\", type=int, default=200, required=False)\n",
        "    parser.add_argument(\"--steps_per_epoch\", type=int, default=None, required=False)\n",
        "    parser.add_argument(\"--patience\", type=int, default=3, required=False)\n",
        "    parser.add_argument(\"--filename\", type=str, default=\"exp006\", required=False)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  Utils\n",
        "# ===============================================================\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# 任意のメッセージを通知する関数\n",
        "def send_slack_message_notification(message):\n",
        "    webhook_url = ' [URL] '  \n",
        "    data = json.dumps({'text': message})\n",
        "    headers = {'content-type': 'application/json'}\n",
        "    requests.post(webhook_url, data=data, headers=headers)\n",
        "    \n",
        "    \n",
        "def seed_everything(cfg):\n",
        "    \"\"\"set seed\"\"\"\n",
        "    random.seed(cfg.seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
        "    np.random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    torch.cuda.manual_seed(cfg.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def data_load(cfg):\n",
        "    \"\"\"loading data, rename columns, fillna text\"\"\"\n",
        "    print(\"========== Data Loading ==========\")\n",
        "    df_content = pd.read_csv(cfg.input_dir+\"content.csv\").fillna({\"title\": \"\", \"description\": \"\", \"text\":\"\"})\n",
        "    df_topics = pd.read_csv(cfg.input_dir+\"topics.csv\").fillna({\"title\": \"\", \"description\": \"\"})\n",
        "    df_corr = pd.read_csv(cfg.input_dir+\"correlations.csv\")   \n",
        "    \n",
        "    df_content.rename(columns={\"id\":\"content_id\"}, inplace=True)\n",
        "    df_topics.rename(columns={\"id\":\"topic_id\"}, inplace=True)\n",
        "    df_corr.rename(columns={\"content_ids\":\"content_id\"}, inplace=True)\n",
        "    \n",
        "    cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model, is_fast=True)\n",
        "    \n",
        "    df_content[\"content_sentence\"] = df_content[\"title\"] + cfg.tokenizer.sep_token + df_content[\"description\"]\n",
        "    df_topics[\"topic_sentence\"] = df_topics[\"title\"] + cfg.tokenizer.sep_token +  df_topics[\"description\"] +\\\n",
        "    cfg.tokenizer.sep_token + df_topics[\"context\"]\n",
        "    df_topics[\"topic_sentence\"] = df_topics[\"topic_sentence\"].str.replace(\" >> \",  \" \")\n",
        "    df_topics = pd.merge(df_topics, df_corr, on=\"topic_id\", how=\"left\")\n",
        "    \n",
        "    \n",
        "    df_topics[\"content_id\"] = df_topics[\"content_id\"].str.split()\n",
        "    df_topics = df_topics.explode(\"content_id\", ignore_index=True)\n",
        "    \n",
        "    df_topics = pd.merge(df_topics, df_content[[\"content_id\", \"content_sentence\"]], on=\"content_id\", how=\"left\")\n",
        "    \n",
        "    print(df_topics.shape)\n",
        "    # category == sourceのtopic\n",
        "    df_train = df_topics[(df_topics[\"category\"] == \"source\")&(df_topics[\"has_content\"] == True)].reset_index(drop=True)\n",
        "    \n",
        "    # category == \n",
        "    df_valid = df_topics[(df_topics[\"category\"] != \"source\")&(df_topics[\"has_content\"] == True)].reset_index(drop=True)\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    split cv using GroupKFold\n",
        "    - split based on number of target's content_id or topic_id\n",
        "    \"\"\"\n",
        "    print(\"========== CV split ==========\")\n",
        "    gkf = GroupKFold(cfg.n_splits)\n",
        "    for i, (tr, val) in enumerate(gkf.split(X=df_valid, groups=df_valid[\"channel\"])):\n",
        "        df_valid.loc[val, \"fold\"] = i\n",
        "    print(df_valid.groupby(\"fold\").size())\n",
        "    print(df_valid[[\"topic_id\", \"fold\"]].drop_duplicates().groupby(\"fold\").size())\n",
        "    \n",
        "    _df_train = df_valid[df_valid[\"fold\"] != cfg.trn_fold].reset_index(drop=True)\n",
        "    df_valid = df_valid[df_valid[\"fold\"] == cfg.trn_fold].reset_index(drop=True)\n",
        "    \n",
        "    df_train = pd.concat([df_train, _df_train], ignore_index=True)\n",
        "\n",
        "    print(\"df_content\", df_content.shape)\n",
        "    print(\"df_corr\", df_corr.shape)\n",
        "    print(\"df_train\", df_train.shape)\n",
        "    print(\"df_valid\", df_valid.shape)\n",
        "    \n",
        "    print(\"Input Sentence Example\")\n",
        "    print(\"========== Topics ==========\")\n",
        "    print(df_topics[\"topic_sentence\"].values.tolist()[:2])\n",
        "    print(\"========== Content ==========\")\n",
        "    print(df_content[\"content_sentence\"].values.tolist()[:2])\n",
        "    print(set(df_valid[\"channel\"].values.tolist())&set(df_train[\"channel\"].values.tolist()))\n",
        "    return df_content, df_train, df_valid\n",
        "\n",
        "# ===============================================================\n",
        "#  Prepare_df\n",
        "# ===============================================================\n",
        "def prepare_train(train):\n",
        "    train = train\n",
        "    train_df = train[[\"topic_sentence\", \"content_sentence\"]]\n",
        "    dataset = Dataset.from_pandas(train_df)\n",
        "    print(dataset)\n",
        "    train_examples = []\n",
        "    n_examples = dataset.num_rows\n",
        "    for i in tqdm(range(n_examples)):\n",
        "        example = dataset[i]\n",
        "        if example[\"topic_sentence\"] == None or example[\"content_sentence\"] == None: #remove None\n",
        "            print(example)\n",
        "            continue        \n",
        "        train_examples.append(InputExample(texts=[str(example[\"topic_sentence\"]), str(example[\"content_sentence\"])]))\n",
        "    return train_examples\n",
        "\n",
        "def prepare_valid(df_content, df_valid):\n",
        "    \"\"\"\n",
        "    Create a query and corpus like the folloing.\n",
        "    \n",
        "    ex)\n",
        "    queries = {'q1': 'What is machine learning?',\n",
        "               'q2': 'How does deep learning work?'}\n",
        "    corpus = {'d1': 'Machine learning is a method of data analysis.', \n",
        "              'd2': 'Deep learning is a subfield of machine learning.', \n",
        "              'd3': 'Neural networks are used in deep learning.'}\n",
        "    relevant_docs = {'q1': {'d1'}, \n",
        "                     'q2': {'d2', 'd3'}}\n",
        "\n",
        "    evaluator = evaluation.InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n",
        "    \"\"\"\n",
        "    queries = df_valid[[\"topic_id\", \"topic_sentence\"]].set_index('topic_id').to_dict()['topic_sentence']\n",
        "    corpus = df_content[[\"content_id\", \"content_sentence\"]].set_index('content_id').to_dict()['content_sentence']\n",
        "    relevant_docs =  df_valid.groupby('topic_id')['content_id'].apply(set).to_dict()    \n",
        "    return queries, corpus, relevant_docs\n",
        "\n",
        "def prepare_df(df_content, df_train, df_valid):\n",
        "    train_examples = prepare_train(df_train)\n",
        "    queries, corpus, relevant_docs = prepare_valid(df_content, df_valid)\n",
        "    return train_examples, queries, corpus, relevant_docs\n",
        "\n",
        "# ===============================================================\n",
        "# Helper functions\n",
        "# ===============================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    #print(a_norm.shape)\n",
        "    #print(b_norm.shape)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "\n",
        "\n",
        "def dot_score(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "    #print(a.shape)\n",
        "    #print(b.shape)\n",
        "    return torch.mm(a, b.transpose(0, 1))\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    earlystoppingクラス\n",
        "    ref: https://qiita.com/ku_a_i/items/ba33c9ce3449da23b503\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=3):\n",
        "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
        "\n",
        "        self.patience = patience    #設定ストップカウンタ\n",
        "        self.counter = 0            #現在のカウンタ値\n",
        "        self.best_score = None      #ベストスコア\n",
        "        self.early_stop = False     #ストップフラグ\n",
        "\n",
        "    def __call__(self, score, epoch, steps):\n",
        "        \"\"\"\n",
        "        特殊(call)メソッド\n",
        "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
        "        \"\"\"\n",
        "        if self.best_score is None:  #1Epoch目の処理\n",
        "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
        "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
        "            self.counter += 1   #ストップカウンタを+1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
        "            send_slack_message_notification(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
        "                self.early_stop = True\n",
        "        else:  #ベストスコアを更新した場合\n",
        "            self.best_score = score  #ベストスコアを上書き\n",
        "            self.counter = 0  #ストップカウンタリセット\n",
        "\n",
        "# ===============================================================\n",
        "#  CustomTransformer\n",
        "# ===============================================================\n",
        "class CustomTransformer(SentenceTransformer):\n",
        "    def fit(self,\n",
        "            cfg,\n",
        "            train_objectives: Iterable[Tuple[DataLoader, nn.Module]],\n",
        "            evaluator: SentenceEvaluator = None,\n",
        "            epochs: int = 1,\n",
        "            steps_per_epoch = None,\n",
        "            scheduler: str = 'WarmupLinear',\n",
        "            warmup_steps: int = 10000,\n",
        "            optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n",
        "            optimizer_params : Dict[str, object]= {'lr': 2e-5},\n",
        "            weight_decay: float = 0.01,\n",
        "            evaluation_steps: int = 0,\n",
        "            output_path: str = None,\n",
        "            save_best_model: bool = True,\n",
        "            max_grad_norm: float = 1,\n",
        "            use_amp: bool = False,\n",
        "            callback: Callable[[float, int, int], None] = None,\n",
        "            show_progress_bar: bool = True,\n",
        "            checkpoint_path: str = None,\n",
        "            checkpoint_save_steps: int = 500,\n",
        "            checkpoint_save_total_limit: int = 0\n",
        "            ):\n",
        "        \"\"\"\n",
        "        クラスの継承\n",
        "        変更点\n",
        "        * tqdmの中にtqdmを入れるのをなくした\n",
        "        * scoreが表示されるようにした\n",
        "        \"\"\"\n",
        "        \n",
        "        info_loss_functions =  []\n",
        "        for dataloader, loss in train_objectives:\n",
        "            info_loss_functions.extend(ModelCardTemplate.get_train_objective_info(dataloader, loss))\n",
        "        info_loss_functions = \"\\n\\n\".join([text for text in info_loss_functions])\n",
        "\n",
        "        info_fit_parameters = json.dumps({\"evaluator\": fullname(evaluator), \"epochs\": epochs, \"steps_per_epoch\": steps_per_epoch, \"scheduler\": scheduler, \"warmup_steps\": warmup_steps, \"optimizer_class\": str(optimizer_class),  \"optimizer_params\": optimizer_params, \"weight_decay\": weight_decay, \"evaluation_steps\": evaluation_steps, \"max_grad_norm\": max_grad_norm }, indent=4, sort_keys=True)\n",
        "        self._model_card_text = None\n",
        "        self._model_card_vars['{TRAINING_SECTION}'] = ModelCardTemplate.__TRAINING_SECTION__.replace(\"{LOSS_FUNCTIONS}\", info_loss_functions).replace(\"{FIT_PARAMETERS}\", info_fit_parameters)\n",
        "\n",
        "\n",
        "        if use_amp:\n",
        "            from torch.cuda.amp import autocast\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        self.to(self._target_device)\n",
        "\n",
        "        dataloaders = [dataloader for dataloader, _ in train_objectives]\n",
        "\n",
        "        # Use smart batching\n",
        "        for dataloader in dataloaders:\n",
        "            dataloader.collate_fn = self.smart_batching_collate\n",
        "\n",
        "        loss_models = [loss for _, loss in train_objectives]\n",
        "        for loss_model in loss_models:\n",
        "            loss_model.to(self._target_device)\n",
        "\n",
        "        self.best_score = -9999999\n",
        "\n",
        "        if steps_per_epoch is None or steps_per_epoch == 0:\n",
        "            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n",
        "\n",
        "        num_train_steps = int(steps_per_epoch * epochs)\n",
        "\n",
        "        # Prepare optimizers\n",
        "        optimizers = []\n",
        "        schedulers = []\n",
        "        for loss_model in loss_models:\n",
        "            param_optimizer = list(loss_model.named_parameters())\n",
        "\n",
        "            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "            ]\n",
        "\n",
        "            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
        "            scheduler_obj = self._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n",
        "\n",
        "            optimizers.append(optimizer)\n",
        "            schedulers.append(scheduler_obj)\n",
        "\n",
        "\n",
        "        global_step = 0\n",
        "        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n",
        "\n",
        "        num_train_objectives = len(train_objectives)\n",
        "        skip_scheduler = False\n",
        "        for epoch in range(epochs):\n",
        "            losses = AverageMeter()\n",
        "            training_steps = 0\n",
        "            if epoch == 0:\n",
        "                # self._eval_during_training(evaluator, output_path, save_best_model, -1, training_steps, callback)\n",
        "                pass\n",
        "            start = end = time.time()\n",
        "\n",
        "            for loss_model in loss_models:\n",
        "                loss_model.zero_grad()\n",
        "                loss_model.train()\n",
        "\n",
        "            for step in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
        "                for train_idx in range(num_train_objectives):\n",
        "                    loss_model = loss_models[train_idx]\n",
        "                    optimizer = optimizers[train_idx]\n",
        "                    scheduler = schedulers[train_idx]\n",
        "                    data_iterator = data_iterators[train_idx]\n",
        "\n",
        "                    try:\n",
        "                        data = next(data_iterator)\n",
        "                    except StopIteration:\n",
        "                        data_iterator = iter(dataloaders[train_idx])\n",
        "                        data_iterators[train_idx] = data_iterator\n",
        "                        data = next(data_iterator)\n",
        "\n",
        "                    features, labels = data\n",
        "                    labels = labels.to(self._target_device)\n",
        "                    batch_size = labels.size(0)\n",
        "                    features = list(map(lambda batch: batch_to_device(batch, self._target_device), features))\n",
        "               \n",
        "                    if use_amp:\n",
        "                        with autocast():\n",
        "                            loss_value = loss_model(features, labels)\n",
        "                        losses.update(loss_value.item(), batch_size)\n",
        "\n",
        "                        scale_before_step = scaler.get_scale()\n",
        "                        scaler.scale(loss_value).backward()\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        grad_norm = torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        skip_scheduler = scaler.get_scale() != scale_before_step\n",
        "                    else:\n",
        "                        loss_value = loss_model(features, labels)\n",
        "                        losses.update(loss_value.item(), batch_size)\n",
        "                        loss_value.backward()\n",
        "                        grad_norm = torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        optimizer.step()\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if not skip_scheduler:\n",
        "                        scheduler.step()\n",
        "                    end = time.time()\n",
        "                    if step % cfg.print_freq == 0 or step == (steps_per_epoch-1):\n",
        "                        print('Epoch: [{0}][{1}/{2}] '\n",
        "                              'Elapsed {remain:s} '\n",
        "                              'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                              'Grad: {grad_norm:.4f}  '\n",
        "                              'LR: {lr:.8f}  '\n",
        "                              .format(epoch+1, step, steps_per_epoch, \n",
        "                                      remain=timeSince(start, float(step+1)/steps_per_epoch),\n",
        "                                      loss=losses,\n",
        "                                      grad_norm=grad_norm,\n",
        "                                      lr=scheduler.get_lr()[0]))\n",
        "\n",
        "                training_steps += 1\n",
        "                global_step += 1\n",
        "\n",
        "\n",
        "                if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n",
        "                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n",
        "\n",
        "                    for loss_model in loss_models:\n",
        "                        loss_model.zero_grad()\n",
        "                        loss_model.train()\n",
        "\n",
        "                if checkpoint_path is not None and checkpoint_save_steps is not None and checkpoint_save_steps > 0 and global_step % checkpoint_save_steps == 0:\n",
        "                    self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "\n",
        "            self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n",
        "\n",
        "        if evaluator is None and output_path is not None:   #No evaluator, but output path: save final model version\n",
        "            self.save(output_path)\n",
        "\n",
        "        if checkpoint_path is not None:\n",
        "            self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "    \n",
        "    def _eval_during_training(self, evaluator, output_path, save_best_model, epoch, steps, callback):\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        eval_path = output_path\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "            eval_path = os.path.join(output_path, \"eval\")\n",
        "            os.makedirs(eval_path, exist_ok=True)\n",
        "\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, output_path=eval_path, epoch=epoch, steps=steps)\n",
        "            send_slack_message_notification(f\"Epoch: [{epoch+1}] recall@k: {score}\")\n",
        "            print(f\"Epoch: [{epoch+1}] recall@k:\", score)\n",
        "            if callback is not None:\n",
        "                callback(score, epoch, steps)\n",
        "                if callback.early_stop:\n",
        "                    print(\"Early Stopping!\")\n",
        "                    send_slack_message_notification(f\"Early Stopping!\")\n",
        "                    sys.exit()\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                print('\\033[32m'+f\"Save the best model: score {self.best_score}\"+'\\033[0m')\n",
        "                if save_best_model:\n",
        "                    self.save(output_path)\n",
        "    \n",
        "# ===============================================================\n",
        "#  CustomEvaluation\n",
        "# ===============================================================\n",
        "class CustomEvaluator(evaluation.InformationRetrievalEvaluator):\n",
        "    \"\"\"\n",
        "    クラスの継承\n",
        "    \n",
        "    map@k -> recall@kに\n",
        "    \"\"\"\n",
        "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1, *args, **kwargs) -> float:\n",
        "        if epoch != -1:\n",
        "            out_txt = \" after epoch {}:\".format(epoch) if steps == -1 else \" in epoch {} after {} steps:\".format(epoch, steps)\n",
        "        else:\n",
        "            out_txt = \":\"\n",
        "\n",
        "        #logger.info(\"Information Retrieval Evaluation on \" + self.name + \" dataset\" + out_txt)\n",
        "\n",
        "        scores = self.compute_metrices(model, *args, **kwargs)\n",
        "\n",
        "        # Write results to disc\n",
        "        if output_path is not None and self.write_csv:\n",
        "            csv_path = os.path.join(output_path, self.csv_file)\n",
        "            if not os.path.isfile(csv_path):\n",
        "                fOut = open(csv_path, mode=\"w\", encoding=\"utf-8\")\n",
        "                fOut.write(\",\".join(self.csv_headers))\n",
        "                fOut.write(\"\\n\")\n",
        "\n",
        "            else:\n",
        "                fOut = open(csv_path, mode=\"a\", encoding=\"utf-8\")\n",
        "\n",
        "            output_data = [epoch, steps]\n",
        "            for name in self.score_function_names:\n",
        "                for k in self.accuracy_at_k:\n",
        "                    output_data.append(scores[name]['accuracy@k'][k])\n",
        "\n",
        "                for k in self.precision_recall_at_k:\n",
        "                    output_data.append(scores[name]['precision@k'][k])\n",
        "                    output_data.append(scores[name]['recall@k'][k])\n",
        "\n",
        "                for k in self.mrr_at_k:\n",
        "                    output_data.append(scores[name]['mrr@k'][k])\n",
        "\n",
        "                for k in self.ndcg_at_k:\n",
        "                    output_data.append(scores[name]['ndcg@k'][k])\n",
        "\n",
        "                for k in self.map_at_k:\n",
        "                    output_data.append(scores[name]['map@k'][k])\n",
        "\n",
        "            fOut.write(\",\".join(map(str, output_data)))\n",
        "            fOut.write(\"\\n\")\n",
        "            fOut.close()\n",
        "\n",
        "        if self.main_score_function is None:\n",
        "            return max([scores[name]['recall@k'][max(self.precision_recall_at_k)] for name in self.score_function_names])\n",
        "        else:\n",
        "            return scores[self.main_score_function]['recall@k'][max(self.precision_recall_at_k)]\n",
        "\n",
        "            \n",
        "# ===============================================================\n",
        "#  run\n",
        "# ===============================================================\n",
        "        \n",
        "def run(cfg, train_examples, queries, corpus, relevant_docs):\n",
        "    # model\n",
        "    print(f\"========== finetuning fold{cfg.trn_fold} ==========\")\n",
        "    word_embedding_model = models.Transformer(cfg.model, max_seq_length=cfg.max_len)\n",
        "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')\n",
        "    model = CustomTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    \n",
        "    # train\n",
        "    train_dataloader =  datasets.NoDuplicatesDataLoader(train_examples, batch_size=cfg.batch_size)\n",
        "    train_loss = losses.MultipleNegativesSymmetricRankingLoss(model)\n",
        "    warmup_steps = math.ceil(len(train_dataloader) * cfg.epochs * 0.1)\n",
        "\n",
        "    earlystopping = EarlyStopping(patience=cfg.patience)\n",
        "    \n",
        "    # evaluation\n",
        "    evalu = CustomEvaluator(queries, corpus, relevant_docs, corpus_chunk_size = 10000,\n",
        "                                                     precision_recall_at_k = [50],\n",
        "                                                     show_progress_bar = True,\n",
        "                                                     batch_size = cfg.batch_size, \n",
        "                                                     name= \"Evalution\", \n",
        "                                                     write_csv = True, \n",
        "                                                     score_functions = {'cos_sim': cos_sim, 'dot_score': dot_score}\n",
        "                                                     )\n",
        "    \n",
        "    model.fit(cfg,\n",
        "            train_objectives=[(train_dataloader, train_loss)],\n",
        "            evaluator=evalu,\n",
        "            epochs=cfg.epochs,\n",
        "            steps_per_epoch=cfg.steps_per_epoch,\n",
        "            scheduler=\"warmupcosine\",\n",
        "            warmup_steps=warmup_steps,\n",
        "            output_path=cfg.output_dir+cfg.model.replace(\"/\", \"-\")+f\"_fine-tuned\",\n",
        "            save_best_model=True,\n",
        "            use_amp=True,\n",
        "            show_progress_bar=False,\n",
        "            callback=earlystopping\n",
        "            )\n",
        "    \n",
        "# ===============================================================\n",
        "#  main\n",
        "# ===============================================================\n",
        "def main(cfg):\n",
        "    df_content, df_train, df_valid = data_load(cfg)\n",
        "    train_examples, queries, corpus, relevant_docs = prepare_df(df_content, df_train, df_valid)\n",
        "    del df_content, df_train, df_valid\n",
        "    run(cfg, train_examples, queries, corpus, relevant_docs)\n",
        "    \n",
        "# ===============================================================\n",
        "#  Execution\n",
        "# ===============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    args.output_dir = args.output_dir+args.filename+\"/fold_\"+str(args.trn_fold)+\"/1st/\"\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    for k, v in vars(args).items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    main(args)"
      ],
      "id": "1f6c2cef"
    },
    {
      "cell_type": "code",
      "source": [
        "!python 1st_stage_model_train.py\\\n",
        "--model sentence-transformers/all-mpnet-base-v2\\\n",
        "--batch_size 256"
      ],
      "metadata": {
        "id": "NYPTDmsoqaZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dfa5942-8d8d-4845-9324-e8111293e6cf"
      },
      "id": "NYPTDmsoqaZD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-02 15:50:05.478945: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-02 15:50:05.479061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-02 15:50:05.479082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "device: cuda\n",
            "seed: 42\n",
            "input_dir: /content/drive/MyDrive/KAGGLE-LECR/\n",
            "output_dir: /content/drive/MyDrive/KAGGLE-LECR/exp006/fold_0/1st/\n",
            "model: sentence-transformers/all-mpnet-base-v2\n",
            "batch_size: 256\n",
            "max_len: 128\n",
            "epochs: 30\n",
            "trn_fold: 0\n",
            "n_splits: 3\n",
            "print_freq: 200\n",
            "steps_per_epoch: None\n",
            "patience: 3\n",
            "filename: exp006\n",
            "========== Data Loading ==========\n",
            "(295374, 14)\n",
            "========== CV split ==========\n",
            "fold\n",
            "0.0    34239\n",
            "1.0    34194\n",
            "2.0    34196\n",
            "dtype: int64\n",
            "fold\n",
            "0.0     7190\n",
            "1.0     6995\n",
            "2.0    10818\n",
            "dtype: int64\n",
            "df_content (154047, 9)\n",
            "df_corr (61517, 2)\n",
            "df_train (245680, 15)\n",
            "df_valid (34239, 15)\n",
            "Input Sentence Example\n",
            "========== Topics ==========\n",
            "['Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите', 'Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите']\n",
            "========== Content ==========\n",
            "['Sumar números de varios dígitos: 48,029+233,930 </s>Suma 48,029+233,930 mediante el algoritmo estándar.\\n\\n', 'Trovare i fattori di un numero</s>Sal trova i fattori di 120.\\n\\n']\n",
            "set()\n",
            "Dataset({\n",
            "    features: ['topic_sentence', 'content_sentence'],\n",
            "    num_rows: 245680\n",
            "})\n",
            "100% 245680/245680 [00:13<00:00, 17574.68it/s]\n",
            "========== finetuning fold0 ==========\n",
            "Epoch: [1][0/959] Elapsed 0m 1s (remain 22m 45s) Loss: 2.0930(2.0930) Grad: 9.5569  LR: 0.00000001  \n",
            "Epoch: [1][200/959] Elapsed 1m 43s (remain 6m 29s) Loss: 1.4083(1.8216) Grad: 13.0564  LR: 0.00000136  \n",
            "Epoch: [1][400/959] Elapsed 3m 25s (remain 4m 45s) Loss: 1.1174(1.5484) Grad: 4.4640  LR: 0.00000275  \n",
            "Epoch: [1][600/959] Elapsed 5m 7s (remain 3m 3s) Loss: 0.8935(1.3755) Grad: 3.5976  LR: 0.00000414  \n",
            "Epoch: [1][800/959] Elapsed 6m 49s (remain 1m 20s) Loss: 0.8197(1.2558) Grad: 6.9417  LR: 0.00000553  \n",
            "Epoch: [1][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.6091(1.1807) Grad: 3.2635  LR: 0.00000662  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.50it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.08s/it]\n",
            "Epoch: [1] recall@k: 0.7205103795928515\n",
            "\u001b[32mSave the best model: score 0.7205103795928515\u001b[0m\n",
            "Epoch: [2][0/959] Elapsed 0m 0s (remain 8m 55s) Loss: 0.8241(0.8241) Grad: 3.7777  LR: 0.00000663  \n",
            "Epoch: [2][200/959] Elapsed 1m 43s (remain 6m 29s) Loss: 0.6906(0.7103) Grad: 3.3177  LR: 0.00000802  \n",
            "Epoch: [2][400/959] Elapsed 3m 25s (remain 4m 45s) Loss: 0.6271(0.6791) Grad: 3.1082  LR: 0.00000941  \n",
            "Epoch: [2][600/959] Elapsed 5m 7s (remain 3m 3s) Loss: 0.5471(0.6521) Grad: 2.9534  LR: 0.00001080  \n",
            "Epoch: [2][800/959] Elapsed 6m 50s (remain 1m 20s) Loss: 0.4724(0.6245) Grad: 3.4966  LR: 0.00001219  \n",
            "Epoch: [2][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.4900(0.6060) Grad: 2.9237  LR: 0.00001329  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.50it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.08s/it]\n",
            "Epoch: [2] recall@k: 0.7658817403329681\n",
            "\u001b[32mSave the best model: score 0.7658817403329681\u001b[0m\n",
            "Epoch: [3][0/959] Elapsed 0m 0s (remain 8m 33s) Loss: 0.3693(0.3693) Grad: 2.9436  LR: 0.00001330  \n",
            "Epoch: [3][200/959] Elapsed 1m 42s (remain 6m 28s) Loss: 0.4887(0.4561) Grad: 2.8145  LR: 0.00001468  \n",
            "Epoch: [3][400/959] Elapsed 3m 25s (remain 4m 45s) Loss: 0.3402(0.4476) Grad: 2.3287  LR: 0.00001607  \n",
            "Epoch: [3][600/959] Elapsed 5m 8s (remain 3m 3s) Loss: 0.4323(0.4381) Grad: 2.4822  LR: 0.00001746  \n",
            "Epoch: [3][800/959] Elapsed 6m 50s (remain 1m 20s) Loss: 0.3890(0.4275) Grad: 2.5942  LR: 0.00001885  \n",
            "Epoch: [3][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.3310(0.4186) Grad: 2.3914  LR: 0.00001995  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.50it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.09s/it]\n",
            "Epoch: [3] recall@k: 0.7895659032952421\n",
            "\u001b[32mSave the best model: score 0.7895659032952421\u001b[0m\n",
            "Epoch: [4][0/959] Elapsed 0m 0s (remain 8m 44s) Loss: 0.2601(0.2601) Grad: 2.3885  LR: 0.00001996  \n",
            "Epoch: [4][200/959] Elapsed 1m 42s (remain 6m 27s) Loss: 0.3675(0.3383) Grad: 6.2052  LR: 0.00002000  \n",
            "Epoch: [4][400/959] Elapsed 3m 25s (remain 4m 45s) Loss: 0.3619(0.3330) Grad: 2.4655  LR: 0.00001999  \n",
            "Epoch: [4][600/959] Elapsed 5m 7s (remain 3m 3s) Loss: 0.4220(0.3284) Grad: 2.5025  LR: 0.00001997  \n",
            "Epoch: [4][800/959] Elapsed 6m 50s (remain 1m 20s) Loss: 0.3427(0.3223) Grad: 2.4924  LR: 0.00001995  \n",
            "Epoch: [4][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.2983(0.3173) Grad: 2.3265  LR: 0.00001993  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.11s/it]\n",
            "Epoch: [4] recall@k: 0.8031290638037858\n",
            "\u001b[32mSave the best model: score 0.8031290638037858\u001b[0m\n",
            "Epoch: [5][0/959] Elapsed 0m 0s (remain 8m 39s) Loss: 0.2321(0.2321) Grad: 2.2626  LR: 0.00001993  \n",
            "Epoch: [5][200/959] Elapsed 1m 42s (remain 6m 27s) Loss: 0.2308(0.2667) Grad: 2.2426  LR: 0.00001990  \n",
            "Epoch: [5][400/959] Elapsed 3m 25s (remain 4m 45s) Loss: 0.2417(0.2642) Grad: 2.1588  LR: 0.00001987  \n",
            "Epoch: [5][600/959] Elapsed 5m 7s (remain 3m 3s) Loss: 0.2510(0.2612) Grad: 2.1177  LR: 0.00001982  \n",
            "Epoch: [5][800/959] Elapsed 6m 50s (remain 1m 20s) Loss: 0.2382(0.2592) Grad: 1.9183  LR: 0.00001977  \n",
            "Epoch: [5][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.2640(0.2559) Grad: 1.9116  LR: 0.00001973  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.09s/it]\n",
            "Epoch: [5] recall@k: 0.819231266677053\n",
            "\u001b[32mSave the best model: score 0.819231266677053\u001b[0m\n",
            "Epoch: [6][0/959] Elapsed 0m 0s (remain 8m 39s) Loss: 0.2194(0.2194) Grad: 1.9097  LR: 0.00001973  \n",
            "Epoch: [6][200/959] Elapsed 1m 43s (remain 6m 28s) Loss: 0.2676(0.2264) Grad: 3.0034  LR: 0.00001967  \n",
            "Epoch: [6][400/959] Elapsed 3m 25s (remain 4m 46s) Loss: 0.1937(0.2235) Grad: 1.9410  LR: 0.00001961  \n",
            "Epoch: [6][600/959] Elapsed 5m 8s (remain 3m 3s) Loss: 0.2155(0.2216) Grad: 2.0960  LR: 0.00001954  \n",
            "Epoch: [6][800/959] Elapsed 6m 50s (remain 1m 20s) Loss: 0.2601(0.2209) Grad: 1.8510  LR: 0.00001946  \n",
            "Epoch: [6][958/959] Elapsed 8m 11s (remain 0m 0s) Loss: 0.1964(0.2193) Grad: 1.6717  LR: 0.00001940  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.50it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:25<00:00,  9.08s/it]\n",
            "Epoch: [6] recall@k: 0.8259852727115484\n",
            "\u001b[32mSave the best model: score 0.8259852727115484\u001b[0m\n",
            "Epoch: [7][0/959] Elapsed 0m 0s (remain 8m 42s) Loss: 0.1500(0.1500) Grad: 1.4898  LR: 0.00001940  \n",
            "Epoch: [7][200/959] Elapsed 1m 42s (remain 6m 28s) Loss: 0.1790(0.1940) Grad: 1.9471  LR: 0.00001931  \n",
            "Epoch: [7][400/959] Elapsed 3m 25s (remain 4m 46s) Loss: 0.2016(0.1936) Grad: 1.9684  LR: 0.00001922  \n",
            "Epoch: [7][600/959] Elapsed 5m 7s (remain 3m 3s) Loss: 0.2672(0.1921) Grad: 2.4271  LR: 0.00001913  \n",
            "Epoch: [7][800/959] Elapsed 6m 48s (remain 1m 20s) Loss: 0.2180(0.1931) Grad: 3.8510  LR: 0.00001903  \n",
            "Epoch: [7][958/959] Elapsed 8m 9s (remain 0m 0s) Loss: 0.1498(0.1915) Grad: 1.6286  LR: 0.00001894  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.03s/it]\n",
            "Epoch: [7] recall@k: 0.8339697202687807\n",
            "\u001b[32mSave the best model: score 0.8339697202687807\u001b[0m\n",
            "Epoch: [8][0/959] Elapsed 0m 0s (remain 8m 35s) Loss: 0.1585(0.1585) Grad: 1.5648  LR: 0.00001894  \n",
            "Epoch: [8][200/959] Elapsed 1m 42s (remain 6m 26s) Loss: 0.1897(0.1765) Grad: 2.1790  LR: 0.00001883  \n",
            "Epoch: [8][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.1362(0.1770) Grad: 1.6769  LR: 0.00001871  \n",
            "Epoch: [8][600/959] Elapsed 5m 5s (remain 3m 1s) Loss: 0.1988(0.1769) Grad: 1.6305  LR: 0.00001859  \n",
            "Epoch: [8][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.1322(0.1757) Grad: 1.5523  LR: 0.00001846  \n",
            "Epoch: [8][958/959] Elapsed 8m 7s (remain 0m 0s) Loss: 0.1490(0.1746) Grad: 1.6839  LR: 0.00001836  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.03s/it]\n",
            "Epoch: [8] recall@k: 0.836746610043049\n",
            "\u001b[32mSave the best model: score 0.836746610043049\u001b[0m\n",
            "Epoch: [9][0/959] Elapsed 0m 0s (remain 8m 27s) Loss: 0.2041(0.2041) Grad: 1.7963  LR: 0.00001836  \n",
            "Epoch: [9][200/959] Elapsed 1m 42s (remain 6m 24s) Loss: 0.1838(0.1574) Grad: 1.5520  LR: 0.00001822  \n",
            "Epoch: [9][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.1577(0.1602) Grad: 1.3999  LR: 0.00001808  \n",
            "Epoch: [9][600/959] Elapsed 5m 5s (remain 3m 1s) Loss: 0.1353(0.1596) Grad: 1.3585  LR: 0.00001794  \n",
            "Epoch: [9][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.1732(0.1594) Grad: 1.6730  LR: 0.00001779  \n",
            "Epoch: [9][958/959] Elapsed 8m 7s (remain 0m 0s) Loss: 0.1738(0.1585) Grad: 1.8155  LR: 0.00001767  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.04s/it]\n",
            "Epoch: [9] recall@k: 0.8363342683133688\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch: [10][0/959] Elapsed 0m 0s (remain 8m 44s) Loss: 0.1292(0.1292) Grad: 1.4539  LR: 0.00001767  \n",
            "Epoch: [10][200/959] Elapsed 1m 42s (remain 6m 24s) Loss: 0.1739(0.1509) Grad: 8.7597  LR: 0.00001751  \n",
            "Epoch: [10][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.1224(0.1471) Grad: 1.1812  LR: 0.00001735  \n",
            "Epoch: [10][600/959] Elapsed 5m 5s (remain 3m 1s) Loss: 0.1459(0.1460) Grad: 1.6115  LR: 0.00001718  \n",
            "Epoch: [10][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.1552(0.1461) Grad: 1.4906  LR: 0.00001701  \n",
            "Epoch: [10][958/959] Elapsed 8m 7s (remain 0m 0s) Loss: 0.1323(0.1454) Grad: 1.5995  LR: 0.00001687  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.50it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.02s/it]\n",
            "Epoch: [10] recall@k: 0.8352826250349134\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch: [11][0/959] Elapsed 0m 0s (remain 8m 41s) Loss: 0.1108(0.1108) Grad: 1.2591  LR: 0.00001687  \n",
            "Epoch: [11][200/959] Elapsed 1m 42s (remain 6m 25s) Loss: 0.1721(0.1343) Grad: 1.4380  LR: 0.00001669  \n",
            "Epoch: [11][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.1526(0.1357) Grad: 1.4604  LR: 0.00001651  \n",
            "Epoch: [11][600/959] Elapsed 5m 4s (remain 3m 1s) Loss: 0.0828(0.1367) Grad: 1.3401  LR: 0.00001632  \n",
            "Epoch: [11][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.1457(0.1382) Grad: 1.6923  LR: 0.00001613  \n",
            "Epoch: [11][958/959] Elapsed 8m 6s (remain 0m 0s) Loss: 0.0920(0.1376) Grad: 1.1795  LR: 0.00001598  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.02s/it]\n",
            "Epoch: [11] recall@k: 0.8462160561372446\n",
            "\u001b[32mSave the best model: score 0.8462160561372446\u001b[0m\n",
            "Epoch: [12][0/959] Elapsed 0m 0s (remain 8m 37s) Loss: 0.1156(0.1156) Grad: 1.2683  LR: 0.00001598  \n",
            "Epoch: [12][200/959] Elapsed 1m 41s (remain 6m 23s) Loss: 0.1648(0.1266) Grad: 2.2900  LR: 0.00001579  \n",
            "Epoch: [12][400/959] Elapsed 3m 23s (remain 4m 42s) Loss: 0.1572(0.1278) Grad: 1.3603  LR: 0.00001559  \n",
            "Epoch: [12][600/959] Elapsed 5m 4s (remain 3m 1s) Loss: 0.1800(0.1282) Grad: 1.6589  LR: 0.00001538  \n",
            "Epoch: [12][800/959] Elapsed 6m 45s (remain 1m 19s) Loss: 0.1423(0.1280) Grad: 1.5495  LR: 0.00001518  \n",
            "Epoch: [12][958/959] Elapsed 8m 5s (remain 0m 0s) Loss: 0.1155(0.1272) Grad: 1.7815  LR: 0.00001501  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.02s/it]\n",
            "Epoch: [12] recall@k: 0.8463003371920296\n",
            "\u001b[32mSave the best model: score 0.8463003371920296\u001b[0m\n",
            "Epoch: [13][0/959] Elapsed 0m 0s (remain 8m 40s) Loss: 0.1099(0.1099) Grad: 1.4103  LR: 0.00001501  \n",
            "Epoch: [13][200/959] Elapsed 1m 41s (remain 6m 23s) Loss: 0.1083(0.1200) Grad: 1.2707  LR: 0.00001480  \n",
            "Epoch: [13][400/959] Elapsed 3m 22s (remain 4m 42s) Loss: 0.1204(0.1216) Grad: 1.5228  LR: 0.00001458  \n",
            "Epoch: [13][600/959] Elapsed 5m 3s (remain 3m 1s) Loss: 0.1228(0.1212) Grad: 1.3166  LR: 0.00001437  \n",
            "Epoch: [13][800/959] Elapsed 6m 45s (remain 1m 19s) Loss: 0.0700(0.1218) Grad: 1.1262  LR: 0.00001415  \n",
            "Epoch: [13][958/959] Elapsed 8m 5s (remain 0m 0s) Loss: 0.1098(0.1214) Grad: 1.4009  LR: 0.00001398  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.05s/it]\n",
            "Epoch: [13] recall@k: 0.8426094943888264\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch: [14][0/959] Elapsed 0m 0s (remain 8m 33s) Loss: 0.1517(0.1517) Grad: 1.6680  LR: 0.00001397  \n",
            "Epoch: [14][200/959] Elapsed 1m 41s (remain 6m 23s) Loss: 0.0976(0.1166) Grad: 1.4931  LR: 0.00001375  \n",
            "Epoch: [14][400/959] Elapsed 3m 23s (remain 4m 42s) Loss: 0.1092(0.1175) Grad: 1.3046  LR: 0.00001352  \n",
            "Epoch: [14][600/959] Elapsed 5m 4s (remain 3m 1s) Loss: 0.1135(0.1175) Grad: 1.3618  LR: 0.00001330  \n",
            "Epoch: [14][800/959] Elapsed 6m 45s (remain 1m 20s) Loss: 0.1141(0.1162) Grad: 1.3432  LR: 0.00001307  \n",
            "Epoch: [14][958/959] Elapsed 8m 5s (remain 0m 0s) Loss: 0.1323(0.1159) Grad: 1.4036  LR: 0.00001288  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.03s/it]\n",
            "Epoch: [14] recall@k: 0.8466083587338212\n",
            "\u001b[32mSave the best model: score 0.8466083587338212\u001b[0m\n",
            "Epoch: [15][0/959] Elapsed 0m 0s (remain 8m 37s) Loss: 0.0861(0.0861) Grad: 1.1707  LR: 0.00001288  \n",
            "Epoch: [15][200/959] Elapsed 1m 41s (remain 6m 24s) Loss: 0.1014(0.1089) Grad: 1.2441  LR: 0.00001265  \n",
            "Epoch: [15][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.0982(0.1102) Grad: 1.4448  LR: 0.00001242  \n",
            "Epoch: [15][600/959] Elapsed 5m 5s (remain 3m 1s) Loss: 0.1076(0.1091) Grad: 1.3104  LR: 0.00001218  \n",
            "Epoch: [15][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.0952(0.1099) Grad: 1.3983  LR: 0.00001194  \n",
            "Epoch: [15][958/959] Elapsed 8m 6s (remain 0m 0s) Loss: 0.0884(0.1094) Grad: 1.1183  LR: 0.00001175  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.02s/it]\n",
            "Epoch: [15] recall@k: 0.8504612296331476\n",
            "\u001b[32mSave the best model: score 0.8504612296331476\u001b[0m\n",
            "Epoch: [16][0/959] Elapsed 0m 0s (remain 8m 43s) Loss: 0.1027(0.1027) Grad: 1.3916  LR: 0.00001175  \n",
            "Epoch: [16][200/959] Elapsed 1m 41s (remain 6m 24s) Loss: 0.0864(0.1077) Grad: 1.3221  LR: 0.00001151  \n",
            "Epoch: [16][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.1429(0.1070) Grad: 1.5733  LR: 0.00001127  \n",
            "Epoch: [16][600/959] Elapsed 5m 4s (remain 3m 1s) Loss: 0.0812(0.1062) Grad: 1.2214  LR: 0.00001103  \n",
            "Epoch: [16][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.0924(0.1066) Grad: 1.2250  LR: 0.00001079  \n",
            "Epoch: [16][958/959] Elapsed 8m 6s (remain 0m 0s) Loss: 0.0762(0.1061) Grad: 1.3098  LR: 0.00001060  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.03s/it]\n",
            "Epoch: [16] recall@k: 0.845639802297052\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Epoch: [17][0/959] Elapsed 0m 0s (remain 8m 43s) Loss: 0.0996(0.0996) Grad: 1.1999  LR: 0.00001060  \n",
            "Epoch: [17][200/959] Elapsed 1m 42s (remain 6m 25s) Loss: 0.0632(0.1023) Grad: 0.8913  LR: 0.00001036  \n",
            "Epoch: [17][400/959] Elapsed 3m 23s (remain 4m 43s) Loss: 0.0898(0.1008) Grad: 1.1687  LR: 0.00001011  \n",
            "Epoch: [17][600/959] Elapsed 5m 5s (remain 3m 1s) Loss: 0.0833(0.1016) Grad: 1.2251  LR: 0.00000987  \n",
            "Epoch: [17][800/959] Elapsed 6m 46s (remain 1m 20s) Loss: 0.0551(0.1017) Grad: 0.8593  LR: 0.00000963  \n",
            "Epoch: [17][958/959] Elapsed 8m 6s (remain 0m 0s) Loss: 0.1083(0.1008) Grad: 1.5360  LR: 0.00000944  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.02s/it]\n",
            "Epoch: [17] recall@k: 0.8473761036754711\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Epoch: [18][0/959] Elapsed 0m 0s (remain 8m 43s) Loss: 0.0687(0.0687) Grad: 1.0900  LR: 0.00000944  \n",
            "Epoch: [18][200/959] Elapsed 1m 41s (remain 6m 23s) Loss: 0.1424(0.0976) Grad: 1.4051  LR: 0.00000919  \n",
            "Epoch: [18][400/959] Elapsed 3m 23s (remain 4m 42s) Loss: 0.1071(0.0972) Grad: 1.1919  LR: 0.00000895  \n",
            "Epoch: [18][600/959] Elapsed 5m 4s (remain 3m 1s) Loss: 0.0899(0.0992) Grad: 1.5030  LR: 0.00000871  \n",
            "Epoch: [18][800/959] Elapsed 6m 45s (remain 1m 20s) Loss: 0.1080(0.0984) Grad: 1.1104  LR: 0.00000847  \n",
            "Epoch: [18][958/959] Elapsed 8m 5s (remain 0m 0s) Loss: 0.1199(0.0978) Grad: 1.3497  LR: 0.00000828  \n",
            "Batches: 100% 29/29 [00:08<00:00,  3.51it/s]\n",
            "Corpus Chunks: 100% 16/16 [02:24<00:00,  9.03s/it]\n",
            "Epoch: [18] recall@k: 0.8473833398639634\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early Stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "dthuz4WWvJc6"
      },
      "id": "dthuz4WWvJc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF1CI2FpB4jW"
      },
      "outputs": [],
      "source": [],
      "id": "JF1CI2FpB4jW"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUkeuDvFvKOI"
      },
      "id": "WUkeuDvFvKOI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 39578.842447,
      "end_time": "2023-02-09T12:58:22.913513",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-09T01:58:44.071066",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
