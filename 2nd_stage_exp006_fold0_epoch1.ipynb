{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1677792999300,"user":{"displayName":"込山湧士","userId":"15682721970485417690"},"user_tz":-540},"id":"95d4oa6B-8AO","outputId":"1a0c1a69-bda3-45af-dd33-0bba8aa413bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar  2 21:36:38 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    25W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23096,"status":"ok","timestamp":1677793022394,"user":{"displayName":"込山湧士","userId":"15682721970485417690"},"user_tz":-540},"id":"GNKo5TtEUWgD","outputId":"2db951c6-552d-4ed7-fa04-254fd52b55a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12852,"status":"ok","timestamp":1677793035244,"user":{"displayName":"込山湧士","userId":"15682721970485417690"},"user_tz":-540},"id":"QUJoRH3SUPR0","outputId":"0daa5802-5d94-475b-f580-6c7463fc4da8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers[sentencepiece]\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: tokenizers, sentencepiece, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets\n","Successfully installed datasets-2.10.1 dill-0.3.6 huggingface-hub-0.12.1 multiprocess-0.70.14 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1 xxhash-3.2.0\n"]}],"source":["%pip install datasets transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677793035244,"user":{"displayName":"込山湧士","userId":"15682721970485417690"},"user_tz":-540},"id":"gj99NGJDUrck","outputId":"5968adf2-f5cf-47da-8454-f82d0f722f45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing 2nd_stage_model_training.py\n"]}],"source":["%%writefile 2nd_stage_model_training.py\n","\n","# ===============================================================\n","#  Library\n","# ===============================================================\n","import os\n","import gc\n","import math\n","import time\n","import json\n","import pickle\n","import random\n","import requests\n","import argparse\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","from collections import OrderedDict\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","import transformers\n","transformers.logging.set_verbosity_error()\n","from transformers import AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"device: {device}\")\n","\n","# ===============================================================\n","#  args\n","# ===============================================================\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--seed\", type=int, default=42, required=False)\n","    parser.add_argument(\"--input_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/last_data/\", \n","                        required=False, help=\" oof data path \")\n","    parser.add_argument(\"--data_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/\", \n","                        required=False, help=\" competition data path \")\n","    parser.add_argument(\"--output_dir\", type=str, default=\"/content/drive/MyDrive/KAGGLE-LECR/last_data/\", required=False)\n","    parser.add_argument(\"--trn_fold\", type=int, default=0, required=False)\n","    parser.add_argument(\"--filename\", type=str, default=\"exp004\", required=False)\n","    parser.add_argument(\"--batch_size\", type=int, default=96, required=False)\n","    parser.add_argument(\"--max_len\", type=int, default=256, required=False)\n","    parser.add_argument(\"--num_workers\", type=int, default=0, required=False)\n","    parser.add_argument(\"--base_model\", type=str, default=\"sentence-transformers/all-MiniLM-L6-v2\", required=False)\n","    parser.add_argument(\"--target_cols\", type=str, nargs=\"*\", default=[\"target\"], required=False)\n","    parser.add_argument(\"--encoder_lr\", type=float, default=2e-5, required=False)\n","    parser.add_argument(\"--decoder_lr\", type=float, default=2e-5, required=False)\n","    parser.add_argument(\"--eps\", type=float, default=1e-6, required=False)\n","    parser.add_argument(\"--betas\", type=float, default=(0.9, 0.999), required=False)\n","    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, required=False)\n","    parser.add_argument(\"--max_grad_norm\", type=float, default=0.012, required=False)\n","    parser.add_argument(\"--num_cycles\", type=float, default=0.5, required=False)\n","    parser.add_argument(\"--epochs\", type=int, default=4, required=False)\n","    parser.add_argument(\"--scheduler\", type=str, default=\"cosine\", choices=[\"cosine\", \"linear\"], required=False)\n","    parser.add_argument(\"--print_freq\", type=int, default=2000, required=False)\n","    parser.add_argument(\"--weight_decay\", type=float, default=0.01, required=False)\n","    parser.add_argument(\"--num_warmup_steps_ratio\", type=float, default=0.1, required=False)\n","    parser.add_argument(\"--patience\", type=int, default=3, required=False)\n","    parser.add_argument(\"--steps_per_epoch\", type=int, default=None, required=False)\n","    parser.add_argument(\"--save_freq\", type=int, default=2, required=False)\n","    parser.add_argument(\"--debug\", action=\"store_true\", required=False)\n","    parser.add_argument(\"--apex\", action=\"store_false\", required=False)\n","    parser.add_argument(\"--resume\", action=\"store_true\", required=False)\n","    args = parser.parse_args()\n","    args.input_dir = args.input_dir + f\"1st/{args.filename}/fold{args.trn_fold}/\"\n","    args.output_dir = args.output_dir + f\"2nd/{args.filename}/fold{args.trn_fold}/\"\n","    args.model = args.input_dir + args.base_model.replace(\"/\", \"-\")  + \"_fine-tuned/\"\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    return args\n","\n","\n","# ===============================================================\n","#  Utils\n","# ===============================================================\n","\n","# 任意のメッセージを通知する関数\n","def send_slack_message_notification(message):\n","    webhook_url = ' [URL] '  \n","    data = json.dumps({'text': message})\n","    headers = {'content-type': 'application/json'}\n","    requests.post(webhook_url, data=data, headers=headers)\n","\n","\n","def seed_everything(cfg):\n","    \"\"\"set seed\"\"\"\n","    random.seed(cfg.seed)\n","    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n","    np.random.seed(cfg.seed)\n","    torch.manual_seed(cfg.seed)\n","    torch.cuda.manual_seed(cfg.seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, _ in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs\n","\n","\n","\n","class EarlyStopping:\n","    \"\"\"\n","    ref: https://qiita.com/ku_a_i/items/ba33c9ce3449da23b503\n","    \"\"\"\n","    def __init__(self, cfg):\n","        self.cfg = cfg\n","        self.patience = self.cfg.patience\n","        self.counter = 0      \n","        self.best_score = None     \n","        self.early_stop = False\n","\n","    def __call__(self, score):\n","        if self.best_score is None:\n","            self.best_score = score\n","        elif score <= self.best_score:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            send_slack_message_notification(f'[{self.cfg.filename}:fold{self.cfg.trn_fold}] EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.counter = 0\n","\n","\n","def fbeta_score(y_true_ids: pd.Series, y_pred_ids: pd.Series, beta=2, eps=1e-15):\n","    \"\"\"\n","    Args:\n","        y_true_ids: true labels\n","        y_pred_ids: predictions\n","\n","    It is assumed that the above two are in the same topic order.\n","    \"\"\"\n","    true_ids = y_true_ids.str.split()\n","    pred_ids = y_pred_ids.str.split()\n","    score_list = []\n","    for true, pred in zip(true_ids, pred_ids):\n","        TP = (set(true) & set(pred))\n","        try: # predictions があるとき\n","            precision = len(TP) / len(pred)\n","            recall = len(TP) / len(true)\n","            f2 = (1+beta**2) * (precision*recall) / ((beta**2)*precision+recall+eps)\n","            score_list.append(f2)\n","        except: # predictions がないとき\n","            score_list.append(0)\n","    score = sum(score_list) / len(score_list)\n","    return score\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def get_f2_score(predictions, valid_folds):\n","    valid_folds[\"sigmoid\"] = sigmoid(predictions)\n","    best_score = -np.inf\n","    best_thrshold = 0\n","    df_corr = pd.read_csv(\"/content/drive/MyDrive/KAGGLE-LECR/correlations.csv\")\n","    topic = valid_folds[[\"topic_id\"]].drop_duplicates().reset_index(drop=True) # valid_foldsに含まれるtopic_id\n","\n","    best_topic = v\n","    with tqdm(np.arange(0.001, 0.5, 0.001), desc=\"Search best threshold\") as pbar:\n","        for thre in pbar:\n","            valid_folds[\"pred\"] = np.where(valid_folds[\"sigmoid\"] > thre, 1, 0)        \n","            pred_1 = valid_folds[valid_folds[\"pred\"] == 1].reset_index(drop=True)        \n","            topic_true = pd.DataFrame(pred_1.groupby(\"topic_id\")[\"predictions\"].agg(list)).reset_index()\n","            topic_true = pd.merge(topic, topic_true, on=\"topic_id\", how=\"left\").fillna(\" \") # predictionsがないものも考慮する\n","            topic_true[\"predictions\"] = topic_true[\"predictions\"].apply(lambda x: \" \".join(x))\n","            topic_true = pd.merge(topic_true, df_corr, on=\"topic_id\", how=\"left\")\n","            score = fbeta_score(topic_true[\"content_ids\"], topic_true[\"predictions\"])        \n","            if score > best_score:\n","                best_score = score\n","                best_threshold = thre\n","                pbar.set_postfix(OrderedDict(best_score=score, best_threshold = thre))\n","    return best_score, best_threshold\n","\n","\n","def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","            'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","    return optimizer_parameters\n","\n","\n","def get_scheduler(cfg, optimizer, num_train_steps):\n","    if cfg.scheduler == 'linear':\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","        )\n","    elif cfg.scheduler == 'cosine':\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer, num_warmup_steps=cfg.num_warmup_steps, \n","            num_training_steps=num_train_steps, \n","            num_cycles=cfg.num_cycles\n","        )\n","    return scheduler\n","\n","\n","# ===============================================================\n","#  DataLoading\n","# ===============================================================\n","\n","def check_contains(row):\n","    \"\"\"Check whether the content_id is included in the label. Set to 1 if included, 0 if not.\"\"\"\n","    try: # row[\"content_ids\"]が空でないとき\n","        ground_truth = set(row[\"content_ids\"])\n","        pred = row[\"predictions\"]\n","        if pred in ground_truth:\n","            return 1\n","        else:\n","            return 0\n","    except: # row[\"content_ids\"]が空の時\n","        return 0\n","    \n","\n","def determine(df_topics):\n","    \"\"\"\n","    Determine if the extracted content is correct\n","    \"\"\"\n","    df = df_topics[[\"topic_id\", \"predictions\"]]\n","    df.loc[:, \"predictions\"] = df[\"predictions\"].str.split()\n","    df = df.explode(\"predictions\", ignore_index=True)\n","    df = pd.merge(df,\n","                    df_topics[[\"topic_id\", \"content_ids\"]],\n","                    on=\"topic_id\", how=\"left\")\n","    df.loc[:, \"content_ids\"] = df[\"content_ids\"].apply(lambda x: x.split())\n","    tqdm.pandas()\n","    df.loc[:, \"target\"] = df.progress_apply(check_contains, axis=1)\n","    df.drop(\"content_ids\", axis=1, inplace=True)\n","    return df\n","\n","\n","def tokenize(cfg, text, text_pair):\n","    \"\"\"encode to sort by token_length\"\"\"\n","    inputs = cfg.tokenizer.encode_plus(\n","        text,\n","        text_pair,\n","        return_tensors = None, \n","        add_special_tokens = True, \n","        # pad_to_max_length = True,\n","        max_length = cfg.max_len,\n","        truncation = True\n","    )[\"input_ids\"]\n","    return inputs\n","\n","\n","def prepare_df(cfg, data_type):\n","    print(f\"{data_type} loading...\")\n","    if data_type == \"train\":\n","        df = pd.read_csv(f\"/content/drive/MyDrive/KAGGLE-LECR/last_data/1st/{cfg.filename}/fold{cfg.trn_fold}/df_train_for_{cfg.filename}.csv\")\n","        \n","    elif data_type == \"validation\":\n","        df = pd.read_csv(f\"/content/drive/MyDrive/KAGGLE-LECR/last_data/1st/{cfg.filename}/fold{cfg.trn_fold}/df_valid_for_{cfg.filename}.csv\")\n","        df = df.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n","\n","    if cfg.debug:\n","        df = df.sample(n=200, random_state=42).reset_index(drop=True)\n","    \n","    print(f'Input Example[0]: \\n  topic  {df[\"topic_sentence\"].values[0]}\\n content  {df[\"content_sentence\"].values[0]}')\n","    print(f'Input Example[1]: \\n  topic  {df[\"topic_sentence\"].values[1]}\\n content  {df[\"content_sentence\"].values[1]}')\n","    return df\n","\n","\n","def load_data(cfg):\n","    df_train = prepare_df(cfg, data_type=\"train\")\n","    df_valid = prepare_df(cfg, data_type=\"validation\")\n","\n","    print(\"train: \\n\", df_train[\"target\"].value_counts())\n","    print(\"valid: \\n\", df_valid[\"target\"].value_counts())\n","    return df_train, df_valid\n","\n","\n","# ===============================================================\n","#  tokenizer\n","# ===============================================================\n","def tokenizer(cfg):\n","    cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model, is_fast=True)\n","    cfg.tokenizer.save_pretrained(cfg.output_dir+'tokenizer/')\n","    return \n","\n","# ===============================================================\n","#  Dataset\n","# ===============================================================\n","def prepare_input(cfg, text, text_pair):\n","    inputs = cfg.tokenizer.encode_plus(\n","        text, \n","        text_pair,\n","        return_tensors = None, \n","        add_special_tokens = True, \n","        max_length = cfg.max_len,\n","        pad_to_max_length = True,\n","        truncation = True\n","    )\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype = torch.long)\n","    return inputs\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.topic_sentence = df[\"topic_sentence\"].values\n","        self.content_sentence = df[\"content_sentence\"].values\n","        self.target = df[\"target\"].values\n","    def __len__(self):\n","        return len(self.target)\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.topic_sentence[item], text_pair=self.content_sentence[item])\n","        label = torch.tensor(self.target[item], dtype = torch.float)\n","        return inputs, label\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs\n","\n","\n","# ===============================================================\n","#  Model\n","# ===============================================================\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","            self.config.hidden_dropout = 0\n","            self.config.hidden_dropout_prob = 0\n","            self.config.attention_dropout = 0\n","            self.config.attetnion_probs_dropout_prob = 0\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        try:\n","            self.model.gradient_checkpointing_enable()\n","        except:\n","            print(f'{cfg.model} does not support gradient checkpoint.')\n","        self.fc = nn.Linear(self.config.hidden_size, len(cfg.target_cols))\n","        self._init_weights(self.fc)\n","        self.pooling = MeanPooling()\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.paddding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","            \n","    def forward(self, inputs):\n","        transformer_out = self.model(**inputs)\n","        last_hidden_state = transformer_out.last_hidden_state\n","        feature = self.pooling(last_hidden_state, inputs['attention_mask'])\n","        output = self.fc(feature)\n","        return output\n","\n","\n","# ===============================================================\n","#  _loop_fn\n","# ===============================================================\n","def train_fn(cfg, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled = cfg.apex)\n","    losses = AverageMeter()\n","    start = time.time()\n","    global_step = 0\n","    for step, (inputs, label) in enumerate(train_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        label = label.to(device)\n","        batch_size = label.size(0)\n","        with torch.cuda.amp.autocast(enabled = cfg.apex):\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds.view(-1), label)\n","            if cfg.gradient_accumulation_steps > 1:\n","                loss = loss / cfg.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad()\n","        global_step += 1\n","        scheduler.step()\n","        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'maxLR: {maxlr:.8f}  '\n","                  'minLR: {maxlr:.8f}  '\n","                  .format(epoch + 1, \n","                          step, \n","                          len(train_loader), \n","                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n","                          loss = losses,\n","                          grad_norm = grad_norm,\n","                          maxlr = scheduler.get_lr()[0],\n","                          minlr = scheduler.get_lr()[0])\n","                 )\n","            send_slack_message_notification(\"[{filename}:fold{trn_fold}] \"\n","                                            'Epoch: [{epoch}][{step}/{le}] '\n","                                            'Elapsed {remain:s} '\n","                                            'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                                            'Grad: {grad_norm:.4f}  '\n","                                            'maxLR: {maxlr:.8f}  '\n","                                            'minLR: {maxlr:.8f}  '\n","                                            .format(filename = cfg.filename,\n","                                                    trn_fold = cfg.trn_fold,\n","                                                    epoch = epoch + 1, \n","                                                    step = step, \n","                                                    le = len(train_loader), \n","                                                    remain = timeSince(start, float(step + 1) / len(train_loader)),\n","                                                    loss = losses,\n","                                                    grad_norm = grad_norm,\n","                                                    maxlr = scheduler.get_lr()[0],\n","                                                    minlr = scheduler.get_lr()[0]))\n","    return losses.avg\n","\n","\n","def valid_fn(cfg, valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = time.time()\n","    for step, (inputs, target) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        target = target.to(device)\n","        batch_size = target.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds.view(-1), target)\n","            if cfg.gradient_accumulation_steps > 1:\n","                loss = loss / cfg.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.squeeze().to('cpu').numpy().reshape(-1))\n","        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, \n","                          len(valid_loader),\n","                          loss = losses,\n","                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n","            send_slack_message_notification(\"[{filename}:fold{trn_fold}] \"\n","                                            'EVAL: [{step}/{le}] '\n","                                            'Elapsed {remain:s} '\n","                                            'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                                            .format(filename = cfg.filename,\n","                                                    trn_fold = cfg.trn_fold,\n","                                                    step = step, \n","                                                    le = len(valid_loader),\n","                                                    loss = losses,\n","                                                    remain = timeSince(start, float(step + 1) / len(valid_loader))))\n","    predictions = np.concatenate(preds, axis = 0)\n","    return losses.avg, predictions\n","\n","# ===============================================================\n","#  loop_fn\n","# ===============================================================\n","\n","def train_loop(cfg, df_train, df_valid, fold, device):\n","    \n","    print(f\"========== fold: {fold} training ==========\")\n","\n","    print(\"train.shape: \", df_train.shape)\n","    print(\"valid.shape: \", df_valid.shape)\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_dataset = TrainDataset(cfg, df_train)\n","    valid_dataset = TrainDataset(cfg, df_valid)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=cfg.batch_size,\n","                              shuffle=True,\n","                              num_workers=cfg.num_workers,\n","                              pin_memory=True,\n","                              drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=cfg.batch_size,\n","                              shuffle=False,\n","                              num_workers=cfg.num_workers,\n","                              pin_memory=True,\n","                              drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(cfg, config_path=None, pretrained=True)\n","    torch.save(model.config, cfg.output_dir+'config.pth')\n","    \n","    model.to(device)\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=cfg.encoder_lr, \n","                                                decoder_lr=cfg.decoder_lr,\n","                                                weight_decay=cfg.weight_decay)\n","\n","    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n","\n","    #for name, p in model.named_parameters():\n","    #   print(name, p.requires_grad)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================    \n","    if cfg.steps_per_epoch is None:\n","        num_train_steps=len(train_loader)*(cfg.epochs)//cfg.gradient_accumulation_steps\n","    else:\n","        num_train_steps=cfg.steps_per_epoch*(cfg.epochs)//cfg.gradient_accumulation_steps\n","\n","    cfg.num_warmup_steps=int((len(train_loader)*(cfg.epochs)//cfg.gradient_accumulation_steps)*cfg.num_warmup_steps_ratio)\n","\n","    callback = EarlyStopping(cfg)\n","    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n","\n","    best_score = -np.inf\n","\n","    if cfg.resume:\n","        checkpoint = torch.load(cfg.output_dir+'model_tmp.pth')\n","        _epoch = checkpoint['epoch']\n","        best_score = checkpoint['best_score']\n","        callback.counter = checkpoint['early_stopping']\n","        callback.best_score = best_score\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        for state in optimizer.state.values():\n","            for k, v in state.items():\n","                if isinstance(v, torch.Tensor):\n","                    state[k] = v.to(device)\n","        scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        print(f\"Resume from Epoch {_epoch+1}\")    \n","    else:\n","        scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","\n","    for epoch in range(cfg.epochs):\n","        if cfg.resume and epoch <= _epoch:\n","            continue\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(cfg, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(cfg, valid_loader, model, criterion, device)\n","\n","        # scoring\n","        #score, thres = get_f2_score(predictions, df_valid)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        #print(f'Epoch {epoch+1} - Score: {score:.4f}  Thres: {thres}')\n","\n","        #send_slack_message_notification(f'[{cfg.filename}:fold{cfg.trn_fold}]  Epoch {epoch+1} - Score: {score:.4f}  Thres: {thres}')\n","        \n","        #print('\\033[32m'+f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model'+'\\033[0m')\n","        #send_slack_message_notification(f'[{cfg.filename}:fold{cfg.trn_fold}]  Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","        torch.save({'model': model.state_dict(),\n","                    'predictions': predictions},\n","                    cfg.output_dir+f\"{cfg.base_model.replace('/', '-')}_fold{fold}_epoch{epoch}_best.pth\")\n","        \n","        predictions = torch.load(cfg.output_dir+f\"{cfg.base_model.replace('/', '-')}_fold{fold}_epoch{epoch}_best.pth\", \n","                                 map_location=torch.device('cpu'))['predictions']\n","        df_valid[\"valid_pred\"] = predictions\n","        df_valid.to_csv(cfg.output_dir+f\"oof_df_fold{cfg.trn_fold}_epoch{epoch}.csv\", index=False)\n","\n","        # 途中保存用\n","        torch.save({'epoch': epoch, \n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    \"best_score\":best_score,\n","                    \"early_stopping\":callback.counter,\n","                    'scheduler_state_dict':scheduler.state_dict()},\n","                cfg.output_dir+'model_tmp.pth')\n","        print(f\"Epoch {epoch+1} Model save is finished.\")\n","        break\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return \n","\n","\n","# ===============================================================\n","#  Main\n","# ===============================================================\n","def main(cfg):\n","    seed_everything(cfg)\n","    tokenizer(cfg)\n","    df_train, df_valid = load_data(cfg)\n","    train_loop(cfg, df_train, df_valid, cfg.trn_fold, device)\n","    return\n","\n","# ===============================================================\n","#  Execute\n","# ===============================================================\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","    for k, v in vars(args).items():\n","        print(f\"{k}: {v}\")\n","    main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4mlHP84-4Nl","outputId":"4510e725-7988-48dd-e3b7-bf49eac06dc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-02 21:37:18.249027: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-02 21:37:19.246722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-02 21:37:19.246858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-02 21:37:19.246886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","device: cuda\n","seed: 42\n","input_dir: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/\n","data_dir: /content/drive/MyDrive/KAGGLE-LECR/\n","output_dir: /content/drive/MyDrive/KAGGLE-LECR/last_data/2nd/exp006/fold0/\n","trn_fold: 0\n","filename: exp006\n","batch_size: 32\n","max_len: 256\n","num_workers: 0\n","base_model: sentence-transformers/all-mpnet-base-v2\n","target_cols: ['target']\n","encoder_lr: 2e-05\n","decoder_lr: 2e-05\n","eps: 1e-06\n","betas: (0.9, 0.999)\n","gradient_accumulation_steps: 1\n","max_grad_norm: 0.012\n","num_cycles: 0.5\n","epochs: 4\n","scheduler: cosine\n","print_freq: 5000\n","weight_decay: 0.01\n","num_warmup_steps_ratio: 0.1\n","patience: 3\n","steps_per_epoch: None\n","save_freq: 2\n","debug: False\n","apex: True\n","resume: False\n","model: /content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/sentence-transformers-all-mpnet-base-v2_fine-tuned/\n","train loading...\n","Input Example[0]: \n","  topic  Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите\n"," content  Предизвикателство с компас</s>Отговори на няколко въпроса за откритията, свързани с компаса\n","Input Example[1]: \n","  topic  Откриването на резисторите</s>Изследване на материали, които предизвикват намаление в отклонението, когато се свържат последователно с нашия измервателен уред. </s>Khan Academy (български език) Наука Физика Открития и проекти Откриването на резисторите\n"," content  Отблъскване на електронните двойки във валентния слой при 4 електронни облака</s>Отблъскване на електронните двойки във валентния слой при 4 електронни облака\n","и примери за тетраедрични, тригонално-пирамидални и огънати молекули.\n","\n","\n","validation loading...\n","Input Example[0]: \n","  topic  Medicine</s></s>Medicine\n"," content  Cancer</s>\n","Input Example[1]: \n","  topic  Medicine</s></s>Medicine\n"," content  AIDS</s>\n","train: \n"," 0    2495837\n","1     245680\n","Name: target, dtype: int64\n","valid: \n"," 0    332563\n","1     26937\n","Name: target, dtype: int64\n","========== fold: 0 training ==========\n","train.shape:  (2741517, 7)\n","valid.shape:  (359500, 8)\n","/content/drive/MyDrive/KAGGLE-LECR/last_data/1st/exp006/fold0/sentence-transformers-all-mpnet-base-v2_fine-tuned/ does not support gradient checkpoint.\n","Epoch: [1][0/85672] Elapsed 0m 3s (remain 5069m 0s) Loss: 0.6909(0.6909) Grad: 1.0325  maxLR: 0.00000000  minLR: 0.00000000  \n","Epoch: [1][5000/85672] Elapsed 40m 17s (remain 650m 0s) Loss: 0.2381(0.3650) Grad: 0.3586  maxLR: 0.00000292  minLR: 0.00000292  \n","Epoch: [1][10000/85672] Elapsed 80m 43s (remain 610m 45s) Loss: 0.2280(0.3164) Grad: 2.2768  maxLR: 0.00000584  minLR: 0.00000584  \n","Epoch: [1][15000/85672] Elapsed 121m 3s (remain 570m 16s) Loss: 0.3274(0.2929) Grad: 2.9196  maxLR: 0.00000876  minLR: 0.00000876  \n","Epoch: [1][20000/85672] Elapsed 161m 24s (remain 529m 56s) Loss: 0.3456(0.2778) Grad: 1.5988  maxLR: 0.00001167  minLR: 0.00001167  \n","Epoch: [1][25000/85672] Elapsed 201m 50s (remain 489m 48s) Loss: 0.1927(0.2672) Grad: 1.6344  maxLR: 0.00001459  minLR: 0.00001459  \n","Epoch: [1][30000/85672] Elapsed 242m 30s (remain 449m 59s) Loss: 0.1936(0.2592) Grad: 4.6053  maxLR: 0.00001751  minLR: 0.00001751  \n","Epoch: [1][35000/85672] Elapsed 283m 13s (remain 410m 1s) Loss: 0.1593(0.2530) Grad: 2.1281  maxLR: 0.00002000  minLR: 0.00002000  \n","Epoch: [1][40000/85672] Elapsed 323m 41s (remain 369m 34s) Loss: 0.2682(0.2478) Grad: 0.9529  maxLR: 0.00001998  minLR: 0.00001998  \n","Epoch: [1][45000/85672] Elapsed 364m 9s (remain 329m 7s) Loss: 0.0739(0.2432) Grad: 1.2933  maxLR: 0.00001994  minLR: 0.00001994  \n","Epoch: [1][50000/85672] Elapsed 404m 40s (remain 288m 41s) Loss: 0.3087(0.2392) Grad: 1.2753  maxLR: 0.00001987  minLR: 0.00001987  \n","Epoch: [1][55000/85672] Elapsed 445m 9s (remain 248m 14s) Loss: 0.1438(0.2358) Grad: 2.6417  maxLR: 0.00001978  minLR: 0.00001978  \n","Epoch: [1][60000/85672] Elapsed 485m 46s (remain 207m 50s) Loss: 0.2664(0.2326) Grad: 3.2742  maxLR: 0.00001966  minLR: 0.00001966  \n","Epoch: [1][65000/85672] Elapsed 526m 25s (remain 167m 24s) Loss: 0.0787(0.2298) Grad: 2.0018  maxLR: 0.00001951  minLR: 0.00001951  \n","Epoch: [1][70000/85672] Elapsed 567m 20s (remain 127m 0s) Loss: 0.1371(0.2272) Grad: 0.7753  maxLR: 0.00001934  minLR: 0.00001934  \n","Epoch: [1][75000/85672] Elapsed 608m 11s (remain 86m 31s) Loss: 0.1272(0.2250) Grad: 2.0934  maxLR: 0.00001915  minLR: 0.00001915  \n","Epoch: [1][80000/85672] Elapsed 648m 49s (remain 45m 59s) Loss: 0.0884(0.2228) Grad: 1.5372  maxLR: 0.00001893  minLR: 0.00001893  \n","Epoch: [1][85000/85672] Elapsed 689m 25s (remain 5m 26s) Loss: 0.0549(0.2209) Grad: 1.2461  maxLR: 0.00001869  minLR: 0.00001869  \n","Epoch: [1][85671/85672] Elapsed 694m 55s (remain 0m 0s) Loss: 0.1803(0.2206) Grad: 1.0396  maxLR: 0.00001866  minLR: 0.00001866  \n","EVAL: [0/11235] Elapsed 0m 0s (remain 7m 50s) Loss: 0.5467(0.5467) \n","EVAL: [5000/11235] Elapsed 8m 2s (remain 10m 1s) Loss: 0.1348(0.1746) \n","EVAL: [10000/11235] Elapsed 24m 3s (remain 2m 58s) Loss: 0.1851(0.1837) \n","EVAL: [11234/11235] Elapsed 30m 43s (remain 0m 0s) Loss: 0.0520(0.1869) \n","Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1869  time: 43540s\n","Epoch 1 Model save is finished.\n"]}],"source":["!python 2nd_stage_model_training.py\\\n","--print_freq 5000\\\n","--base_model sentence-transformers/all-mpnet-base-v2\\\n","--filename exp006\\\n","--batch_size 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPSnVb0ZhM4y"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc7XMdRkDF8R"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}